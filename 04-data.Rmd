```{r knitr, echo=FALSE}
knitr::opts_chunk$set(
  eval      = TRUE,
  comment   = "#",
  results   = "hold",
  message = FALSE,
  warning = FALSE,
  # collapse  = TRUE,
  fig.align = "center")
#library(png) 
library(jpeg)
#library(knitr)
```


# Simple models for synthetic communities {#data}

Ecologists have performed large experiments in which different assemblages of species are co-cultured. These experiments have been conducted with plants (for example, Biodiversity Ecosystem Functioning experiments e.g., [@hector1999plant] [@tilman2001diversity] [@cadotte2013experimental]) and in the laboratory using protozoan, algae or bacteria. Two commonly-encountered problems in this type of experiments have to to with the scaling of the number of experiments with the number of species, and with the probability of coexistence.

**Scale:** How many communities can we form from a pool of $n$ species? We can culture a single species in isolation ($n$ possibilities), two species in pair ($n(n-1) / 2$ possibilities), and so on. The total is therefore:

$$
\sum_{j=1}^n \binom{n}{j} = 2^n -1
$$

And this is only considering the presence/absence of each species! Moreover, we might want to vary the initial conditions (e.g., starting two species at low/high abundance, equal abundance, high/low abundace), etc. Clearly, this makes trying all possible combinations unfeasible when $n$ is large enough. For example, for 10 species we can form 1023 assemblages, while with 20 more than a million!

**Coexistence:** even if we could try all possible experiments, many assemblages would collapse to smaller communities because of extinctions. For example, pairs could become monocultures, triplets become pairs or monocultures, etc. As such, even if we were to try all possible combinations, we would end up observing a smaller set of "final communities". 

To solve these two issues, we would need to find a good way to navigate the enormous space of possibilities, thereby suggesting "good" experiments that yield a large probability of coexistence. 

## Synthetic communities using the GLV model

To set the stage for deriving a statistical model that can deal with the problems above, we start by simulating the communities that can be formed from a pool of $n$ species using the GLV model. First, we need a way to index our communities. We take a vector $p$ reporting the presence/absence of a given species at the beginning of the experiment. We then associate a label $k \in \{1, \ldots, 2^n -1\}$ with the experiment. This label uniquely defines the composition of a community. If only taxon 1 is present ($p = [1,0,0,\ldots,0]$), then $k = 1$, if only taxon 2 is present ($p =[0,1,0,\ldots,0]$) then $k = 2$, if both taxa 1 and 2 are part of the experiment, $k = 3$ ($p = [1,1,0,\ldots,0]$); in general, $k = \sum_{i = 1}^n p_i 2^{(i-1)}$ where $p_i = 1$ if taxon $i$ is part of the experiment, and $p_i = 0$ otherwise.

We can then cycle through each of the $2^n - 1$ assemblages that can be formed, and determine whether the species will coexist or not in our experiment. For simplicity, we report as coexisting any set of species for which a) the GLV equilibrium is feasible, and b) it is locally stable. We collect all the communities that are coexisting along with the abundance of all species in the matrix $E$. 

In `R`:

```{r, randomglvpars}
set.seed(3)
# take a random matrix of interactions: 
# -Aij <- U[0,1]; -Aii <- Sqrt(2) + U[0,1]
# all interactions are competitive
n <- 5
A <- -matrix(runif(n * n), n, n)
diag(A) <- diag(A) - sqrt(2)
print(A)
# choose positive growth rates
r <- runif(n) + 0.5
print(r)
```

Now go through all possible combinations:

```{r, glvbef}
E <- matrix(0, 0, n) # matrix containing all communities
x_template <- rep(0, n) # template for row of E
for (k in 1:(2^n - 1)){
  p <- as.integer(intToBits(k)[1:n])
  presence <- p > 0
  A_k <- A[presence, presence, drop = FALSE]
  r_k <- r[presence]
  # check if equilibrium is feasible
  x_k_star <- solve(A_k, -r_k)
  if (all(x_k_star > 0)){
    # check if equilibrium is locally stable
    # a) build community matrix
    M <- x_k_star * A_k
    # b) compute eigenvalues
    eM <- eigen(M, only.values = TRUE)$values
    # c) check stability
    if (all(Re(eM) < 0)){
      # we have a feasible, stable equilibrium
      # add to the set
      tmp <- x_template
      tmp[presence] <- x_k_star
      E <- rbind(E, tmp)
    }
  }
}
rownames(E) <- NULL
head(E) # show the first few communities
tail(E) # show the last few communities
```

## Total biomass

Our simulated communties have much in common with BEF experiments. For example, plotting the species richness on the x-axis and the total biomass on the y-axis, we recover the typical pattern found in real experiments ([@hector1999plant] [@tilman2001diversity] @cadotte2013experimental]):

```{r, glvbefplot}
library(tidyverse) # plotting and wrangling
total_biomass <- rowSums(E)
species_richness <- rowSums(E > 0)
BEF <- tibble(species_richness = species_richness, total_biomass = total_biomass)
ggplot(data = BEF) + 
  aes(x = species_richness, y = total_biomass) + 
  geom_point() + geom_smooth()
```

showing a modest increase in total biomass with increasing species richness.

## Hyperplanes

We can rewrite the GLV model as:

$$
\begin{aligned}
\dfrac{dx(t)}{dt} &= D(x(t))(r + A x(t))\\
&= D(r x(t))(1 + B x(t))
\end{aligned}
$$
where we have $b_{ij} = a_{ij} / r_i$. The equilibrium for each subset of species $k$ (if it exists) can be found as $(B^{(k)})^{-1} 1^{(k)}$, where $(B^{(k)})^{-1}$ is the inverse of the $B^{(k)}$ sub-matrix of $B$ in which only the rows/columns belonging to $k$ are retained, and $1^{(k)}$ is a vector of ones with length equal to the number of species in $k$. 

We now want to link the matrix $B$ with the results of the experiments, stored in matrix $E$. Call $x^{(k)}_i$ the recorded density of taxon $i$ in community $k$ (i.e., stored in one of the rows of $E$). Then, we can build a matrix $P$ where each row represents an equation $\sum_j b_{ij} x_j^{(k)} = 1$ and we have a column for each coefficient in $A$. For example, take three taxa, and suppose that we can culture each in isolation, and that the all assemblages coexist when co-cultured. We can write 12 equations: monocultures ($k \in \{1, 2, 4\}$) give rise to a single equation; co-cultures of two taxa to two equations each ($k \in \{3, 5, 6 \}$); and communities with three taxa to three independent equations ($k = 7$). Here are the 12 equations we can write:

$$
\begin{align}
b_{11} x_1^{(1)} &= 1\\
b_{22} x_2^{(2)} &= 1\\
b_{11} x_1^{(3)} + b_{12}x_2^{(3)} &= 1\\
b_{21} x_1^{(3)} + b_{22}x_2^{(3)} &= 1\\
b_{33} x_3^{(4)} &= 1\\
b_{11} x_1^{(5)} + b_{13} x_3^{(5)} &= 1\\
b_{31} x_1^{(5)} + b_{33} x_3^{(5)} &= 1\\
b_{22} x_2^{(6)} + b_{23} x_3^{(6)} &= 1\\
b_{32} x_2^{(6)} + b_{33} x_3^{(6)} &= 1\\
b_{11} x_1^{(7)} + b_{12} x_2^{(7)} + b_{13} x_3^{(7)} & = 1\\
b_{21} x_1^{(7)} + b_{22} x_2^{(7)} + b_{23} x_3^{(7)} & = 1\\
b_{31} x_1^{(7)} + b_{32} x_2^{(7)} + b_{33} x_3^{(7)} & = 1
\end{align}
$$

We can summarize the equations in a more compact form as $P v(B)=1$:

$$
\begin{pmatrix}
x_1^{(1)} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & x_2^{(2)} & 0 & 0 & 0 & 0 \\
x_1^{(3)} & x_2^{(3)} & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & x_1^{(3)} & x_2^{(3)} & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & x_3^{(4)} \\
x_1^{(5)} & 0 & x_3^{(5)} & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & x_1^{(5)} & 0 & x_3^{(5)} \\
0 & 0 & 0 & 0 & x_2^{(6)} & x_3^{(6)} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & x_2^{(6)} & x_3^{(6)}\\
x_1^{(7)} & x_2^{(7)} & x_3^{(7)} & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & x_1^{(7)} & x_2^{(7)} & x_3^{(7)} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & x_1^{(7)} & x_2^{(7)} & x_3^{(7)} 
\end{pmatrix}
\begin{pmatrix}
b_{11}\\
b_{12}\\
b_{13}\\
b_{21}\\
b_{22}\\
b_{23}\\
b_{31}\\
b_{32}\\
b_{33}
\end{pmatrix} 
= 
\begin{pmatrix}
1\\
1\\
1\\
1\\
1\\
1\\
1\\
1\\
1\\
1\\
1\\
1
\end{pmatrix}
$$
where $v(B)$ is a vectorized version of $B$. Because we have $n 2^{n-1} = 12$ equations, and $n^2 = 9$ variables, in principle the system can be solved. One can recognize the equation above as a linear regression, and choose $\hat{v}(B) = P^{+}1$ (where $P^{+}$ is the Moore-Penrose pseudo-inverse of $P$). Before doing that, however, we can try to simplify the calculation by rearranging the rows of $P$:

$$
\begin{pmatrix}
x_1^{(1)} & 0 & 0      & 0 & 0 & 0     & 0 & 0 & 0 \\
x_1^{(3)} & x_2^{(3)} & 0      & 0 & 0 & 0     & 0 & 0 & 0 \\
x_1^{(5)} & 0 & x_3^{(5)}       & 0 & 0 & 0     & 0 & 0 & 0 \\
x_1^{(7)} & x_2^{(7)} & x_3^{(7)} & 0 & 0 & 0     & 0 & 0 & 0 \\
0 & 0 & 0     & 0 & x_2^{(2)} & 0     & 0 & 0 & 0 \\
0 & 0 & 0     & x_1^{(3)} & x_2^{(3)} & 0     & 0 & 0 & 0 \\
0 & 0 & 0     & 0 & x_2^{(6)} & x_3^{(6)}     & 0 & 0 & 0 \\
0 & 0 & 0     & x_1^{(7)} & x_2^{(7)} & x_3^{(7)} & 0 & 0 & 0 \\
0 & 0 & 0      & 0 & 0 & 0     & 0 & 0 & x_3^{(4)} \\
0 & 0 & 0     & 0 & 0 & 0     & x_1^{(5)} & 0 & x_3^{(5)} \\
0 & 0 & 0     & 0 & 0 & 0        & 0 & x_2^{(6)} & x_3^{(6)}\\
0 & 0 & 0     & 0 & 0 & 0        & x_1^{(7)} & x_2^{(7)} & x_3^{(7)} 
\end{pmatrix} 
\begin{pmatrix}
b_{11}\\
b_{12}\\
b_{13}\\
b_{21}\\
b_{22}\\
b_{23}\\
b_{31}\\
b_{32}\\
b_{33}
\end{pmatrix} 
= 
\begin{pmatrix}
1\\
1\\
1\\
1\\
1\\
1\\
1\\
1\\
1\\
1\\
1\\
1
\end{pmatrix}
$$

Showing that the matrix $P$ is block-diagonal, with blocks $P_i = E_i$, where $E_i$ is the matrix $E$ in which only the rows in which species $i$ is present are retained. As such, we can solve for the matrix $B$ one row at a time:

$$
\begin{pmatrix}
x_1^{(1)} & 0 & 0 \\
x_1^{(3)} & x_2^{(3)} & 0 \\
x_1^{(5)} & 0 & x_3^{(5)} \\
x_1^{(7)} & x_2^{(7)} & x_3^{(7)} 
\end{pmatrix} 
\begin{pmatrix}
b_{11}\\
b_{12}\\
b_{13}
\end{pmatrix} 
= 
\begin{pmatrix}
1\\
1\\
1\\
1
\end{pmatrix}
$$

For example:

```{r, examplefitendpoints}
# rescaled matrix
B <- 1 / r * A
# now try to recover B from matrix E
fitted_B <- matrix(0, n, n)
for (i in 1:n){
  # take the matrix Ei
  Ei <- E[E[,i] > 0, , drop = FALSE]
  # solve for the row
  fitted_B[i,] <- -rowSums(MASS::ginv(Ei))
}
ggplot(data = tibble(real = as.vector(B),
                     fitted = as.vector(fitted_B))) + 
  aes(x = real, y = fitted) + geom_point() + 
  geom_abline(slope = 1, intercept = 0)
```

Notice that in general the system of equations is over-determined. As such, we can fit using a subset of the experiments, and then predict the rest of the experiments out-of-fit. For example, let's fit again using the information on monocultures and pairs only, and then predict experiment 31, in which all species should coexist:

```{r, examplefitendpoints2}
E_partial <- E[rowSums(E> 0) < 3, ]
fitted_B <- matrix(0, n, n)
for (i in 1:n){
  # take the matrix Ei
  Ei <- E_partial[E_partial[,i] > 0, , drop = FALSE]
  # solve for the row
  fitted_B[i,] <- -rowSums(MASS::ginv(Ei))
}
ggplot(data = tibble(real = as.vector(B),
                     fitted = as.vector(fitted_B))) + 
  aes(x = real, y = fitted) + geom_point() + 
  geom_abline(slope = 1, intercept = 0)
print("predicted")
solve(fitted_B, - rep(1, n))
print("observed")
E[(2^n -1), ]
```

The equation $E_i B_i = -1$ shows that all the equilibria of species $i$ in a GLV system are arranged on a hyperplane.

## Fitting real data

Having played with synthetic data, we can start thinking about data stemming from real experiments. In particular, consider the linear, additive model:

$$
x_i^{(k)} = \gamma_i - \sum_{j\neq i} \theta_{ij} x_i^{(k)}
$$
where $x_i^{(k)}$ is the density of species $i$ in community $k$,  $\gamma_i$ models the density of $i$ when grown in isolation, and the parameters $\theta_{ij}$ how the density of $i$ is influenced by the other species in community $k$. Dividing both sides by $\gamma_i$ and rearranging:

$$
\sum_{j\neq i} (\theta_{ij} /\gamma_i) x_i^{(k)} + 1/{\gamma_i} x_i^{(k)}= \sum_j b_{ij} x_j^{(k)} = 1
$$
which is exactly the same system of equations found above. As such, fitting a linear, addittive model for the biomass of each species in each plot is equivalent to finding the equilibrium strcuture of a GLV model that approximates the data.

One thing to consider when modeling real data, contrary to simulated data, is the structure of the errors. For example, empirical data often contain replicates that reach slightly different densities. Typically, what is done in these types of experiments is to sort and measure biomass for a subset of each plot, and then to multiply to extrapolate the biomass for the whole plot. As such, if the error is normally distributed for the sub-plot, then it's going to be lognormally distributed for the whole plot.





## Predicting real data out of fit


